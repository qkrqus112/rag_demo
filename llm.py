from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import FAISS

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œë¥¼ ìœ„í•´ ì¶”ê°€í•©ë‹ˆë‹¤. (Streamlit ì•± ì‹œì‘ ì‹œ ì´ë¯¸ ë¡œë“œë  ìˆ˜ë„ ìˆì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ)
import os
from dotenv import load_dotenv

# answer_examplesëŠ” config.py íŒŒì¼ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
from config import answer_examples

# .env íŒŒì¼ ë¡œë“œ (Streamlit ì•±ì´ ì‹œì‘ë  ë•Œ í•œ ë²ˆë§Œ í˜¸ì¶œë˜ë„ë¡ ê´€ë¦¬ í•„ìš”)
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì €ì¥í•  ì¸ë©”ëª¨ë¦¬ ìŠ¤í† ì–´ì…ë‹ˆë‹¤.
# ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” Redis, ë°ì´í„°ë² ì´ìŠ¤ ë“±ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """
    ì£¼ì–´ì§„ session_idì— í•´ë‹¹í•˜ëŠ” ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìƒˆë¡œìš´ ì„¸ì…˜ì´ë©´ ChatMessageHistoryë¥¼ ìƒì„±í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


def get_retriever():
    """
    OpenAI ì„ë² ë”©ê³¼ FAISSë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    FAISSëŠ” ë¡œì»¬ ë””ë ‰í† ë¦¬ 'faiss_akis'ì— ì €ì¥ë©ë‹ˆë‹¤.
    """
    embedding = OpenAIEmbeddings(model='text-embedding-3-large', openai_api_key=OPENAI_API_KEY)
    persist_directory = './faiss_akis'
    try:
        database = FAISS.load_local(
            folder_path=persist_directory,
            embeddings=embedding,
            allow_dangerous_deserialization=True  # âœ… ì¶”ê°€!
        )
        if len(database.index_to_docstore_id) == 0:
            print(f"ê²½ê³ : FAISS '{persist_directory}'ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"FAISS ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise e

    return database.as_retriever(search_kwargs={'k': 4})

def get_history_retriever():
    """
    ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì¬êµ¬ì„±í•˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    llm = get_llm() # get_llm í•¨ìˆ˜ëŠ” openai_api_keyë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    retriever = get_retriever() # get_retriever í•¨ìˆ˜ëŠ” openai_api_keyë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        "ì£¼ì–´ì§„ ì±„íŒ… ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ê³ ë ¤í•˜ì—¬, "
        "ì±„íŒ… ê¸°ë¡ì˜ ë§¥ë½ì„ ì°¸ì¡°í•  ìˆ˜ ìˆëŠ” ë…ë¦½ì ì¸ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”. "
        "ì§ˆë¬¸ì— ë‹µí•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ìˆëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."
    )

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¸ì‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"), # ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì‚½ì…ë  ìœ„ì¹˜
            ("human", "{input}"), # í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì‚½ì…ë  ìœ„ì¹˜
        ]
    )
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ì¬êµ¬ì„± ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever


def get_llm(model='gpt-4o'):
    """
    OpenAI Chat ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(model=model, openai_api_key=OPENAI_API_KEY)
    return llm


def get_dictionary_chain():
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ì‚¬ì „(dictionary)ì„ ì°¸ê³ í•˜ì—¬ ë³€ê²½í•˜ëŠ” ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì˜ˆ: 'ìš°ë¦¬ íšŒì‚¬' -> 'AKì•„ì´ì—ìŠ¤'
    """
    dictionary_rules = ["íšŒì‚¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> AKì•„ì´ì—ìŠ¤"] # ì‚¬ì „ì— ë¯¸ë¦¬ ì •ì˜ëœ ê·œì¹™ ë¦¬ìŠ¤íŠ¸
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”.

        ì‚¬ì „: {dictionary_rules}
        
        ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser() # í”„ë¡¬í”„íŠ¸ -> LLM -> ë¬¸ìì—´ íŒŒì‹±
    
    return dictionary_chain


def get_rag_chain():
    """
    RAG(Retrieval Augmented Generation) ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì±„íŒ… íˆìŠ¤í† ë¦¬, Few-shot ì˜ˆì‹œ, ì‚¬ìš©ì ì •ì˜ í”„ë¡¬í”„íŠ¸, ë¦¬íŠ¸ë¦¬ë²„ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    # Few-shot ì˜ˆì‹œë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì…ë‹ˆë‹¤.
    # `config.py`ì˜ `answer_examples`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples, # `config.answer_examples` ì‚¬ìš©
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì±—ë´‡ì˜ ì—­í• , ì§€ì¹¨, context í”Œë ˆì´ìŠ¤í™€ë” í¬í•¨
    system_prompt = (
        "ë‹¹ì‹ ì€ AKì•„ì´ì—ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë³µë¦¬í›„ìƒ, ìŠ¹ì§„ ë§ˆì¼ë¦¬ì§€ ì œë„ ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        "ì•„ë˜ì— ì œê³µëœ ë¬¸ì„œë¥¼ í™œìš©í•´ì„œ ë‹µë³€í•´ì£¼ì‹œê³ ,\n"
        "ë‹µë³€ì„ ì•Œ ìˆ˜ ì—†ë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        "ë‹µë³€ì„ ì œê³µí• ë•Œì—ëŠ” ì´ëª¨í‹°ì½˜ì„ í™œìš©í•´ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”! ğŸ˜Š\n"
        "\n"
        "{context}" # ê²€ìƒ‰ëœ ë¬¸ì„œ(context)ê°€ ì‚½ì…ë  ìœ„ì¹˜
    )
    
    # RAGì˜ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, Few-shot ì˜ˆì‹œ, ì±„íŒ… íˆìŠ¤í† ë¦¬, ì‚¬ìš©ì ì§ˆë¬¸ì„ í¬í•¨í•©ë‹ˆë‹¤.
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt, # Few-shot ì˜ˆì‹œ í¬í•¨
            MessagesPlaceholder("chat_history"), # ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì‚½ì…ë  ìœ„ì¹˜
            ("human", "{input}"), # ì‚¬ìš©ì ì§ˆë¬¸ì´ ì‚½ì…ë  ìœ„ì¹˜
        ]
    )
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¸ì§€ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    history_aware_retriever = get_history_retriever()
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ì…ë‹ˆë‹¤.
    # ë³€ìˆ˜ëª…ì„ 'Youtube_chain'ì—ì„œ ë” ì ì ˆí•œ 'Youtube_generator_chain'ìœ¼ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
    Youtube_generator_chain = create_stuff_documents_chain(llm, qa_prompt)

    # ë¦¬íŠ¸ë¦¬ë²„ì™€ ë‹µë³€ ìƒì„± ì²´ì¸ì„ ê²°í•©í•˜ì—¬ RAG ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.
    rag_chain = create_retrieval_chain(history_aware_retriever, Youtube_generator_chain)
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ë©° RAG ì²´ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” RunnableWithMessageHistoryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history, # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
        input_messages_key="input", # ì‚¬ìš©ì ì§ˆë¬¸ì„ ìœ„í•œ í‚¤
        history_messages_key="chat_history", # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ„í•œ í‚¤
        # output_messages_keyëŠ” Streamlitì˜ st.write_streamì— ì§ì ‘ ì „ë‹¬í•˜ëŠ” ê²½ìš° í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # í•˜ì§€ë§Œ pick('answer')ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ëª…ì‹œí•˜ëŠ” ê²ƒì´ ì¼ê´€ì„±ì„ ë†’ì…ë‹ˆë‹¤.
        output_messages_key="answer", 
    ).pick('answer') # ìµœì¢…ì ìœ¼ë¡œ 'answer' í‚¤ì˜ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •

    return conversational_rag_chain


def get_ai_response(user_message):
    """
    ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë°›ì•„ ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ê³  AI ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì‚¬ì „ ë³€í™˜ ì²´ì¸ê³¼ RAG ì²´ì¸ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()
    
    # ì‚¬ì „ ë³€í™˜ ì²´ì¸ì˜ ì¶œë ¥ì´ RAG ì²´ì¸ì˜ ì…ë ¥ì´ ë˜ë„ë¡ ì—°ê²°í•©ë‹ˆë‹¤.
    # dictionary_chainì˜ ê²°ê³¼ëŠ” RAG chainì˜ 'input'ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
    # LangChain Expression Language (LCEL)ì˜ í‘œí˜„ì‹ ì²´ì¸ì…ë‹ˆë‹¤.
    full_rag_pipeline = {"input": dictionary_chain} | rag_chain
    
    # ì„¸ì…˜ IDë¥¼ 'abc123'ìœ¼ë¡œ ê³ ì •í•˜ì—¬ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” ì‚¬ìš©ìë³„ë¡œ ë™ì ìœ¼ë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    # dictionary_chainì˜ ì…ë ¥ì€ "question"ì´ê³ , ìµœì¢… rag_chainì˜ ì…ë ¥ì€ "input"ì…ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œ 'input'ì€ user_messageê°€ ì•„ë‹Œ, dictionary_chainì˜ ê²°ê³¼ë¥¼ ë°›ê²Œ ë©ë‹ˆë‹¤.
    ai_response = full_rag_pipeline.stream(
        {
            "question": user_message # dictionary_chainìœ¼ë¡œ ì „ë‹¬ë  ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
        },
        config={
            "configurable": {"session_id": "abc123"} # ì„¸ì…˜ ID ì„¤ì • (RunnableWithMessageHistoryìš©)
        },
    )

    return ai_response
