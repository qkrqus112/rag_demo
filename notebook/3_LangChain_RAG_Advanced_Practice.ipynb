{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacd7ca5",
   "metadata": {},
   "source": [
    "# ê³ ê¸‰ RAG ì‹¤ìŠµ ë…¸íŠ¸ë¶ (PDF/í…ìŠ¤íŠ¸/ì±„íŒ…/í“¨ìƒ·/ì‚¬ì „ë³€í™˜)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ì–‘í•œ **ê³ ê¸‰ RAG(Retrieval-Augmented Generation)** ê¸°ë²•ì„ ì‹¤ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "**ì£¼ìš” ì‹¤ìŠµ ë‚´ìš©**\n",
    "- PDF/í…ìŠ¤íŠ¸ ë¬¸ì„œ ì²­í¬í™” ë° ë²¡í„°í™”\n",
    "- ChromaDB ê¸°ë°˜ ì €ì¥ ë° ê²€ìƒ‰\n",
    "- ë‹¤ì–‘í•œ RetrievalQA ì²´ì¸ ê¸°ë²• (stuff, map_reduce, refine)\n",
    "- Chat history ê¸°ë°˜ ConversationalRetrievalChain\n",
    "- LangChain Expression Language(LCEL) ê¸°ë°˜ ì²´ì¸ êµ¬í˜„\n",
    "- Few-shot Prompting(í“¨ìƒ·) ì ìš©\n",
    "- ì…ë ¥ ì‚¬ì „ ë³€í™˜ ì²´ì¸(RunnableLambda ë“±) í™œìš©\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1e185",
   "metadata": {},
   "source": [
    "## 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° í™˜ê²½ ë³€ìˆ˜ ë¡œë”©\n",
    "\n",
    "- ì‹¤ìŠµì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³ , .env íŒŒì¼ì—ì„œ OpenAI API í‚¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85738f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "assert openai_api_key, \"OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d2322",
   "metadata": {},
   "source": [
    "## 2. PDF/í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¡œë”© ë° ì²­í¬ ë¶„í• \n",
    "\n",
    "- ì—¬ëŸ¬ PDF íŒŒì¼ì„ì„ ë¡œë“œí•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5778dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import glob\n",
    "\n",
    "data_dir = \"../data\"\n",
    "pdf_files = glob.glob(os.path.join(data_dir, \"*.pdf\"))\n",
    "\n",
    "all_documents = []\n",
    "for pdf_path in pdf_files:\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    all_documents.extend(docs)\n",
    "    print(f\"{os.path.basename(pdf_path)} ë¬¸ì„œ {len(docs)}ê°œ ë¡œë“œë¨\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=250,\n",
    "    separators=[\"\\n\", \".\", \" \"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(all_documents)\n",
    "print(f\"ì´ ì²­í¬ ê°œìˆ˜: {len(chunks)}\")\n",
    "for chunk in chunks[:3]:\n",
    "    print('-', chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bab58f",
   "metadata": {},
   "source": [
    "## 3. ChromaDB ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ë° ì €ì¥\n",
    "\n",
    "- ë¶„í• ëœ ì²­í¬ ë°ì´í„°ë¥¼ ì„ë² ë”©(ë²¡í„°í™”)í•˜ì—¬ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "- persist_directoryë¥¼ ì§€ì •í•˜ë©´ DBê°€ íŒŒì¼ë¡œ ì €ì¥ë˜ì–´ ì¬ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba7cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', openai_api_key=openai_api_key)\n",
    "persist_dir = \"../chroma\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(chunks, embedding, persist_directory=persist_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f41f2",
   "metadata": {},
   "source": [
    "## 4. RetrievalQA ì²´ì¸ ê¸°ë²• ë¹„êµ (stuff, map_reduce, refine)\n",
    "\n",
    "- RetrievalQA ì²´ì¸ì€ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ LLMì— ì–´ë–»ê²Œ ì „ë‹¬í•˜ê³  ë‹µë³€ì„ ìƒì„±í• ì§€ ë‹¤ì–‘í•œ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- ê° ì²´ì¸ íƒ€ì…ì˜ íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
    "\n",
    "| ì²´ì¸ íƒ€ì…      | ë™ì‘ ë°©ì‹ ìš”ì•½                                                                 | ì¥ì /íŠ¹ì§•                                  |\n",
    "| -------------- | ----------------------------------------------------------------------------- | ------------------------------------------ |\n",
    "| **stuff**      | ëª¨ë“  ê²€ìƒ‰ ë¬¸ì„œë¥¼ í•œ ë²ˆì— LLMì— ì…ë ¥í•˜ì—¬ ë‹µë³€ ìƒì„±                              | ë¬¸ì„œê°€ ì ì„ ë•Œ ë¹ ë¥´ê³  ê°„ë‹¨, ë‹µë³€ì´ ì¼ê´€ë¨   |\n",
    "| **map_reduce** | ê° ë¬¸ì„œë³„ë¡œ LLMì´ ë¶€ë¶„ ë‹µë³€(map) â†’ ë¶€ë¶„ ë‹µë³€ì„ ë‹¤ì‹œ LLMì— ì…ë ¥í•´ ìµœì¢… ë‹µë³€(reduce) | ë¬¸ì„œê°€ ë§ì„ ë•Œ ìœ ë¦¬, ìš”ì•½/í†µí•©ì— ê°•ì        |\n",
    "| **refine**     | ì²« ë¬¸ì„œë¡œ ì´ˆì•ˆ ìƒì„± í›„, ë‚˜ë¨¸ì§€ ë¬¸ì„œë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€í•˜ë©° ë‹µë³€ì„ ì ì§„ì ìœ¼ë¡œ ë³´ì™„ | ë¬¸ì„œë³„ë¡œ ì ì§„ì  ë³´ì™„, ê¸´ ë¬¸ì„œì— ì í•©        |\n",
    "\n",
    "- ì•„ë˜ ì…€ì—ì„œ ê° ì²´ì¸ íƒ€ì…ë³„ë¡œ ê°™ì€ ì§ˆë¬¸ì„ ì…ë ¥í•´ ë‹µë³€ ê²°ê³¼ì™€ ì°¨ì´ë¥¼ ì§ì ‘ ë¹„êµí•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"íœ´ê°€ ê´€ë ¨ëœ ë‚´ìš© ì•Œë ¤ì¤˜!\"\n",
    "\n",
    "for chain_type in [\"stuff\", \"map_reduce\", \"refine\"]:\n",
    "    print(f\"\\n--- RetrievalQA ì²´ì¸ íƒ€ì…: {chain_type} ---\")\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=chain_type,\n",
    "        return_source_documents=False\n",
    "    )\n",
    "    result = chain.invoke({\"query\": query})\n",
    "    print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e48747",
   "metadata": {},
   "source": [
    "## 5. ConversationalRetrievalChain (ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸°ë°˜)\n",
    "\n",
    "- ì‚¬ìš©ìì˜ ì´ì „ ì§ˆë¬¸/ë‹µë³€ íˆìŠ¤í† ë¦¬ë¥¼ ë°˜ì˜í•˜ì—¬ ë¬¸ë§¥ì— ë§ëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- ì‹¤ì œ ì±—ë´‡ ì‹œë‚˜ë¦¬ì˜¤ì— ê°€ê¹Œìš´ ì²´í—˜ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba476fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "questions = [\n",
    "    \"ì¶œì‚°íœ´ê°€ ê´€ë ¨í•´ì„œ ì•Œë ¤ì¤˜!\",\n",
    "    \"ë°°ìš°ìê°€ í•˜ë©´?\",\n",
    "    \"ë§ˆì¼ë¦¬ì§€ ì œë„ëŠ” ì–´ë–»ê²Œ ë¼?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = conv_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"A:\", result[\"answer\"])\n",
    "    chat_history.append((question, result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6025f5e",
   "metadata": {},
   "source": [
    "## 6. Few-shot Prompting(í“¨ìƒ·) ì ìš©\n",
    "\n",
    "- LLM í”„ë¡¬í”„íŠ¸ì— ì˜ˆì‹œ(ìƒ·)ë¥¼ ì¶”ê°€í•˜ì—¬ ì›í•˜ëŠ” ë‹µë³€ ìŠ¤íƒ€ì¼ì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì•„ë˜ ì˜ˆì‹œëŠ” Q/A í˜•ì‹ì˜ ìƒ·ì„ ì¶”ê°€í•˜ì—¬, ë‹µë³€ì˜ ì¼ê´€ì„±ê³¼ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88209f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ì—ì„œ contextì™€ query(ì§ˆë¬¸) ëª¨ë‘ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•´ì•¼ í•˜ë©°,\n",
    "# RetrievalQAì˜ ì…ë ¥ ë³€ìˆ˜ëª…ê³¼ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…ì´ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "fewshot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì¹œê·¼í•œ ë‹µë³€ì„ í•˜ëŠ” í•œêµ­ì–´ ë¹„ì„œì•¼. ì´ëª¨í‹°ì½˜ë„ í™œìš©í•´ì¤˜.\"),\n",
    "    (\"human\", \"íœ´ê°€ ì œë„ ì•Œë ¤ì¤˜!\"),\n",
    "    (\"ai\", \"ë„¤! ğŸ˜Š íœ´ê°€ ì œë„ëŠ” ì—°ì°¨, ì¶œì‚°íœ´ê°€, ê²½ì¡°íœ´ê°€ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆì–´ìš”. ê¶ê¸ˆí•œ ì ì„ ë” ë¬¼ì–´ë´ì£¼ì„¸ìš”!\"),\n",
    "    (\"human\", \"{question}\"), # <-- ì—¬ê¸°ë¥¼ {question}ìœ¼ë¡œ ë³€ê²½\n",
    "    (\"system\", \"ì°¸ê³  ë¬¸ì„œ:\\n{context}\")\n",
    "])\n",
    "\n",
    "print(\"ChatPromptTemplate ì •ì˜ ì™„ë£Œ.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 2. RetrievalQA ì²´ì¸ ì •ì˜ ---\n",
    "# RetrievalQAì—ì„œ promptë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” chain_type=\"stuff\"ë¡œ ëª…ì‹œí•˜ê³ ,\n",
    "# document_variable_nameì„ \"context\"ë¡œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "qa_chain_fewshot = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": fewshot_prompt,\n",
    "        \"document_variable_name\": \"context\" # ì°¸ê³  ë¬¸ì„œ ë³€ìˆ˜ëª…\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"RetrievalQA ì²´ì¸ ì •ì˜ ì™„ë£Œ.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 3. ì§ˆì˜ ì‹¤í–‰ ---\n",
    "# invoke ë©”ì„œë“œì— ì „ë‹¬í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ëŠ” ChatPromptTemplateì—ì„œ ì‚¬ìš©ëœ í”Œë ˆì´ìŠ¤í™€ë” ì´ë¦„ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "# {question}ì— í•´ë‹¹í•˜ëŠ” ê°’ìœ¼ë¡œ \"query\"ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "# RetrievalQAëŠ” ë‚´ë¶€ì ìœ¼ë¡œ \"query\"ë¥¼ \"question\"ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n",
    "# ë§Œì•½ ì—¬ì „íˆ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ invokeì˜ í‚¤ë¥¼ \"question\"ìœ¼ë¡œ ë³€ê²½í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "result = qa_chain_fewshot.invoke({\"query\": \"ë§ˆì¼ë¦¬ì§€ ì œë„ ì•Œë ¤ì¤˜!\"})\n",
    "print(\"\\n--- ê²°ê³¼ ---\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2736199",
   "metadata": {},
   "source": [
    "\n",
    "# 7. ì…ë ¥ ì‚¬ì „ ë³€í™˜ ì²´ì¸(RunnableLambda ë“±) ì‹¤ìŠµ\n",
    "RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ LLMì— ì „ë‹¬í•˜ê±°ë‚˜ ê²€ìƒ‰ê¸°ì— ë„˜ê¸°ê¸° ì „ì—, ì „ì²˜ë¦¬(Pre-processing) ê³¼ì •ì„ ê±°ì³ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. RunnableLambdaì™€ ê°™ì€ LangChainì˜ ìœ ì—°í•œ ì»´í¬ë„ŒíŠ¸ë“¤ì„ í™œìš©í•˜ë©´ ì´ëŸ¬í•œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì²´ì¸ ë‚´ì— ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì…ë ¥ ì‚¬ì „ ë³€í™˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- ì¿¼ë¦¬ ì¬ì‘ì„± (Query Rewriting): ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜, ëŒ€í™” ì´ë ¥ì´ ìˆëŠ” ê²½ìš°, ë˜ëŠ” ê²€ìƒ‰ì— ë¹„íš¨ìœ¨ì ì¸ í‘œí˜„ì„ í¬í•¨í•  ë•Œ, LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ì— ìµœì í™”ëœ ìƒˆë¡œìš´ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ë²¡í„° DB ê²€ìƒ‰ì˜ ì •í™•ë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- í‚¤ì›Œë“œ ì¶”ì¶œ: ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë§Œì„ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ì— í™œìš©í•©ë‹ˆë‹¤.\n",
    "- ì…ë ¥ í¬ë§·íŒ…: ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì…ë ¥ê°’ì„ íŠ¹ì • í˜•ì‹ì— ë§ì¶° ì¡°í•©í•˜ê±°ë‚˜ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "- ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°: ì§ˆë¬¸ ë‚´ì˜ ê°íƒ„ì‚¬, ë¶ˆí•„ìš”í•œ ì„œë¡  ë“±ì„ ì œê±°í•˜ì—¬ í•µì‹¬ë§Œ ë‚¨ê¹ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ ì˜ˆì‹œëŠ” RunnableLambdaì™€ LLMì„ ì¡°í•©í•˜ì—¬, ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ë” ì í•©í•œ í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„±í•œ í›„ ì´ë¥¼ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì²´ì¸ì— ì „ë‹¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŠ” RAG ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì¸ ê³ ê¸‰ ê¸°ë²•ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser # LLM ì¶œë ¥ íŒŒì‹±ìš©\n",
    "\n",
    "# --- 1. ì¿¼ë¦¬ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸ ì •ì˜ ---\n",
    "# ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•˜ê¸° ê°€ì¥ ì¢‹ì€ í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ë„ë¡ LLMì—ê²Œ ì§€ì‹œí•©ë‹ˆë‹¤.\n",
    "query_rewriter_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ë„ˆëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰í•˜ê¸° ê°€ì¥ ì¢‹ì€ í‚¤ì›Œë“œ ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì•¼. ì§ˆë¬¸ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ì—¬ ê°„ê²°í•˜ê³  ëª…í™•í•œ ê²€ìƒ‰ì–´ë§Œ ì¶”ì¶œí•´ì¤˜. ì˜ˆë¥¼ ë“¤ì–´, 'íœ´ê°€ ì œë„ì— ëŒ€í•´ ìì„¸íˆ ì•Œë ¤ì£¼ì„¸ìš”.' -> 'íœ´ê°€ ì œë„', 'íšŒì‚¬ ë³µì§€ì— ëŒ€í•´ ì•Œë ¤ì¤˜' -> 'íšŒì‚¬ ë³µì§€'\"),\n",
    "    (\"human\", \"{original_query}\") # ì›ë³¸ ì§ˆë¬¸ì„ ë°›ì„ í”Œë ˆì´ìŠ¤í™€ë”\n",
    "])\n",
    "\n",
    "# --- 2. ì¿¼ë¦¬ ì¬ì‘ì„± LLM ì²´ì¸ ìƒì„± ---\n",
    "# LLMì„ ì‚¬ìš©í•˜ì—¬ original_queryë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "query_rewriter_chain = query_rewriter_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- 3. ì¿¼ë¦¬ ì¬ì‘ì„± í•¨ìˆ˜ ì •ì˜ (RunnableLambdaì— ì‚¬ìš©ë ) ---\n",
    "def get_rewritten_query_for_chain(inputs):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì¿¼ë¦¬ ì¬ì‘ì„± ì²´ì¸ì„ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    qa_chain_fewshotì´ ê¸°ëŒ€í•˜ëŠ” 'question' í‚¤ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # query_rewriter_chainì€ {original_query}ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë¯€ë¡œ, inputs[\"query\"]ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
    "    rewritten_q = query_rewriter_chain.invoke({\"original_query\": inputs[\"query\"]})\n",
    "    print(f\"DEBUG: ì›ë³¸ ì§ˆë¬¸: '{inputs['query']}' -> ì¬ì‘ì„±ëœ ì¿¼ë¦¬: '{rewritten_q.strip()}'\") # ë””ë²„ê¹… ì¶œë ¥\n",
    "    return {\"question\": rewritten_q.strip()} # RetrievalQAê°€ ê¸°ëŒ€í•˜ëŠ” \"question\" í‚¤ë¡œ ë°˜í™˜\n",
    "\n",
    "# --- 4. ì „ì²´ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•: ì¿¼ë¦¬ ì¬ì‘ì„± -> RAG ì²´ì¸ ---\n",
    "# ì…ë ¥: {\"query\": \"ì‚¬ìš©ì ì§ˆë¬¸\"}\n",
    "# ì¶œë ¥: {\"result\": \"ë‹µë³€\"}\n",
    "qa_chain_preprocess = (\n",
    "    RunnablePassthrough.assign( # ì…ë ¥{\"query\"}ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ë©´ì„œ\n",
    "        # 'question'ì´ë¼ëŠ” ìƒˆë¡œìš´ í‚¤ë¡œ ì¿¼ë¦¬ ì¬ì‘ì„± ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "        # ì´ 'question'ì€ qa_chain_fewshotì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "        question=RunnableLambda(get_rewritten_query_for_chain)\n",
    "    )\n",
    "    | qa_chain_fewshot # ì¬ì‘ì„±ëœ ì¿¼ë¦¬ê°€ í¬í•¨ëœ ì…ë ¥ì´ qa_chain_fewshotìœ¼ë¡œ ì „ë‹¬ë¨\n",
    ")\n",
    "\n",
    "# --- 5. ì§ˆì˜ ì‹¤í–‰ ---\n",
    "result = qa_chain_preprocess.invoke({\"query\": \"ë§ˆì¼ë¦¬ì§€ ì œë„ ì•Œë ¤ì¤˜!\"})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a348d",
   "metadata": {},
   "source": [
    "## 8. LCEL ê¸°ë°˜ ì²´ì¸ êµ¬ì„± (LangChain Expression Language)\n",
    "\n",
    "- LCELì„ í™œìš©í•´ ì²´ì¸ ì¡°í•©ì„ ììœ ë¡­ê²Œ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- ì•„ë˜ ì˜ˆì‹œëŠ” ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ ì¡°í•© â†’ ë‹µë³€ ìƒì„±ì„ ëª¨ë‘ LCELë¡œ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40495b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ë‹¹ì‹ ì€ AKì•„ì´ì—ìŠ¤ì˜ ì¹œê·¼í•œ ì±—ë´‡ ë¹„ì„œì…ë‹ˆë‹¤. ğŸ˜Š\n",
    "    ì œê³µëœ 'ì°¸ê³  ë¬¸ì„œ'ì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.\n",
    "    ë‹¤ìŒ ì§€ì‹œì‚¬í•­ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•˜ì„¸ìš”:\n",
    "    1. ì´ëª¨í‹°ì½˜ì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ì¹œê·¼í•˜ê³  ë¶€ë“œëŸ¬ìš´ ë§íˆ¬ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    2. 'ì°¸ê³  ë¬¸ì„œ'ì— ì—†ëŠ” ë‚´ìš©ì´ë‚˜ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì—ëŠ” 'ë‹µë³€ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "    3. ë‹µë³€ì€ ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "    \n",
    "    ì°¸ê³  ë¬¸ì„œ:\n",
    "    {context}\n",
    "\n",
    "    ì§ˆë¬¸: {question}\n",
    "    ë‹µë³€:\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "combine_documents_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=qa_prompt,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "def input_mapper(inputs):\n",
    "    return {\n",
    "        \"context\": retriever.get_relevant_documents(inputs[\"question\"]),\n",
    "        \"question\": inputs[\"question\"]\n",
    "    }\n",
    "\n",
    "rag_chain = RunnableLambda(input_mapper) | combine_documents_chain\n",
    "\n",
    "try:\n",
    "    response = rag_chain.invoke({\"question\": \"ë§ˆì¼ë¦¬ì§€ëŠ” ëˆ„ê°€ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\"})\n",
    "\n",
    "    print(response[\"answer\"] if isinstance(response, dict) and \"answer\" in response else response)\n",
    "except Exception as e:\n",
    "    print(\"RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
