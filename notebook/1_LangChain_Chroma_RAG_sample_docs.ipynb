{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd2b439",
   "metadata": {},
   "source": [
    "# LangChain + Chroma 기반 RAG 실습 (sample_docs.txt)\n",
    "\n",
    "이 노트북에서는 텍스트 파일을 벡터화하여 Chroma DB에 저장하고,  \n",
    "LangChain을 활용해 RAG(Retrieval-Augmented Generation) 기반 답변 생성을 실습합니다.\n",
    "\n",
    "---\n",
    "**실습 목표**\n",
    "- 텍스트 문서를 로드하고, 검색 효율을 높이기 위해 청크로 분할\n",
    "- 임베딩 및 벡터 DB(Chroma) 구축\n",
    "- 유사도 검색을 통한 문맥 기반 답변 생성\n",
    "- LangChain Prompt 및 RetrievalQA 체인 활용법 익히기\n",
    "\n",
    "---\n",
    "**실습 파일:**  \n",
    "`data/sample_docs.txt` 파일을 대상으로 실습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_docs.txt 파일 로드\n",
    "# TextLoader를 사용해 텍스트 파일을 문서 객체로 불러옵니다.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "data_dir = \"../data\"\n",
    "txt_path = os.path.join(data_dir, \"sample_docs.txt\")\n",
    "\n",
    "# 명시적으로 encoding을 'utf-8'로 지정\n",
    "loader = TextLoader(txt_path, encoding='utf-8')\n",
    "all_documents = loader.load()\n",
    "print(f\"{os.path.basename(txt_path)} 문서 {len(all_documents)}개 로드됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec30417",
   "metadata": {},
   "source": [
    "## 텍스트 스플리터\n",
    "- 긴 문서는 LLM 입력 한계 및 검색 효율을 위해 일정 길이의 청크로 분할합니다.\n",
    "- RecursiveCharacterTextSplitter는 문장, 줄바꿈, 공백 등 다양한 구분자를 활용해 자연스럽게 분할합니다.\n",
    "- chunk_size, chunk_overlap 값을 조절해 분할 granularity를 실험할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d98ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "full_text_content = all_documents[0].page_content\n",
    "\n",
    "chunks = [Document(page_content=line.strip()) for line in full_text_content.splitlines() if line.strip()]\n",
    "\n",
    "print(f\"총 청크 개수: {len(chunks)}\")\n",
    "\n",
    "# --- 3. 분할된 청크 내용 확인 (모든 청크 출력) ---\n",
    "print(\"\\n--- 분할된 청크 내용 ---\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'----- 청크 {i+1} -----')\n",
    "    print(chunk.page_content)\n",
    "    print() # 청크 간 가독성을 위한 줄바꿈 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187849d",
   "metadata": {},
   "source": [
    "# Knowledge Base(지식베이스) 구성을 위한 데이터 생성\n",
    "- 분할된 청크 데이터를 임베딩(벡터화)하여 Chroma DB에 저장합니다.\n",
    "- 임베딩 모델(OpenAIEmbeddings 등)을 활용해 텍스트를 벡터로 변환합니다.\n",
    "- persist_directory를 지정하면 DB가 파일로 저장되어 재사용이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23146be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 환경변수를 불러옴 (.env 파일 필요)\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI에서 제공하는 Embedding Model을 활용해서 `chunk`를 vector화\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Chroma DB에 벡터 저장 (persist_directory로 파일 저장)\n",
    "database = Chroma.from_documents(documents=chunks, embedding=embedding, collection_name='chroma-txt', persist_directory=\"../chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2445e6",
   "metadata": {},
   "source": [
    "# 답변 생성을 위한 Retrieval(검색)\n",
    "- Chroma에 저장한 벡터 데이터를 유사도 검색(similarity_search)으로 불러옵니다.\n",
    "- 검색 쿼리는 sample_docs.txt의 내용과 관련된 질문을 사용합니다.\n",
    "- k 값을 조절해 검색 결과 개수를 실험할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de934023",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'RAG에 대한 정보 알려줘!'\n",
    "\n",
    "# `k` 값을 조절해서 얼마나 많은 데이터를 불러올지 결정\n",
    "retrieved_docs = database.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"[{i}] {doc.page_content[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1d5ce",
   "metadata": {},
   "source": [
    "# Augmentation을 위한 Prompt 활용\n",
    "- 검색된 문맥(retrieved_docs)을 LLM에 효과적으로 전달하기 위해 프롬프트 템플릿을 사용합니다.\n",
    "- LangChain Hub의 `\"rlm/rag-prompt\"`는 context(검색 결과)와 question(질문)을 받아 답변을 생성하는 표준 RAG 프롬프트입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ad8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ffed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5258c",
   "metadata": {},
   "source": [
    "# 답변 생성 (RetrievalQA 체인)\n",
    "- RetrievalQA 체인은 검색 결과와 프롬프트를 LLM에 연결해 RAG 파이프라인을 구성합니다.\n",
    "- query(질문)를 입력하면, 검색 → 프롬프트 조합 → LLM 답변 생성이 자동으로 이루어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm, \n",
    "    retriever=database.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "ai_message = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "print(ai_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
